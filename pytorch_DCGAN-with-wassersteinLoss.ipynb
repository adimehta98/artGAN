{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "artWGAN_128.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adimehta98/artGAN/blob/main/pytorch_DCGAN-with-wassersteinLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERnsDluvJ000"
      },
      "source": [
        "random_seed = 1\n",
        "from numpy.random import seed\n",
        "seed(random_seed)\n",
        "import random\n",
        "random.seed(random_seed)\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import os \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as Vutil\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy-dY2L1J6jW",
        "outputId": "921c57d2-7807-4e63-c3bf-12a8468e315a"
      },
      "source": [
        "reloadModels = True\n",
        "nIter= 5000\n",
        "imageSize = (128,128)\n",
        "batchSize = 128\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3F0dTUjJ9Wa",
        "outputId": "b3030cc4-9f28-41d2-e723-bff7cdc45452"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-c3c10bb9-7cb7-93c1-c3ae-eb862e8c8584)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auSAz3P3J_Kj",
        "outputId": "9d5aff65-157c-4356-f0b0-d03e5f976aa8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Pwprz5KBG5"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/Pytorch DCGAN art ')\n",
        "imageFolder = \"./Sample/pics/\" \n",
        "modelFolder = './Model/'\n",
        "resultsFolder = './Results_WGAN' \n",
        "transform = transforms.Compose([transforms.ToPILImage(),transforms.Resize(imageSize),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "import h5py\n",
        " \n",
        "class dataset_h5(torch.utils.data.Dataset):\n",
        "    def __init__(self, in_file, transform=None):\n",
        "        super(dataset_h5, self).__init__()\n",
        " \n",
        "        self.file = h5py.File(in_file, 'r')\n",
        "        self.transform = transform\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        x = self.file['train_img'][index, ...]\n",
        "        y = self.file['train_labels'][index, ...]\n",
        "        \n",
        "        # Preprocessing each image\n",
        "        if self.transform is not None:\n",
        "            x = self.transform(x)        \n",
        "        \n",
        "        return (x, y), index\n",
        " \n",
        "    def __len__(self):\n",
        "        return self.file['train_img'].shape[0]\n",
        "\n",
        "dataset = dataset_h5(\"./Sample/dataset.hdf5\" ,transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batchSize,drop_last=True,shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDAxCgLcKCBy"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "        \n",
        "\n",
        "class Generator(nn.Module): \n",
        "    def __init__(self): \n",
        "        super(Generator, self).__init__() \n",
        "        self.iterCount = 0\n",
        "        self.main = nn.Sequential( \n",
        "            nn.ConvTranspose2d(in_channels=100, out_channels=512, kernel_size=4, stride=1, padding=0, bias = False), \n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True), \n",
        "            \n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), \n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True), \n",
        "\n",
        "            \n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), \n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.LeakyReLU(0.2, inplace = True), \n",
        "            \n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), \n",
        "            nn.BatchNorm2d(64), \n",
        "            nn.LeakyReLU(0.2, inplace = True), \n",
        "            \n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias = False), \n",
        "            nn.BatchNorm2d(32), \n",
        "            nn.LeakyReLU(0.2, inplace = True), \n",
        "            \n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias = False), \n",
        "            nn.Tanh() \n",
        "        )\n",
        "\n",
        "    def forward(self, input): \n",
        "        output = self.main(input) \n",
        "        return output \n",
        "\n",
        "class Discriminator(nn.Module): \n",
        "    def __init__(self):\n",
        "        self.iterCount = 0\n",
        "        super(Discriminator, self).__init__() \n",
        "        self.main = nn.Sequential( \n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2,padding=1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True), \n",
        "            \n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False), \n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            \n",
        "            nn.Conv2d(128, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            \n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(512), \n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            \n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias = False), \n",
        "            #nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input): \n",
        "        output = self.main(input)\n",
        "        return output.view(-1) #FLattening the result \n",
        "\n",
        "if reloadModels:\n",
        "    gen = torch.load(modelFolder + 'generator_wg.pt')\n",
        "    gen = gen.to(device)\n",
        "    disc = torch.load(modelFolder + 'discriminator_wg.pt')\n",
        "    disc = disc.to(device)\n",
        "else:\n",
        "    gen= Generator().to(device)\n",
        "    gen.apply(weights_init) \n",
        "    print('Generator initialised')\n",
        "    disc = Discriminator().to(device)\n",
        "    disc.apply(weights_init)\n",
        "    print('Discriminator initialised')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FYAGu1iRpwV",
        "outputId": "6e8c499e-32a5-4100-9fc7-8880ac7c7c18"
      },
      "source": [
        "print(f' Models trained upto {gen.iterCount} epochs')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Models trained upto 937 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfXq65hUKNNZ",
        "outputId": "e7027ad5-45db-4904-9d0c-08ed97c32196"
      },
      "source": [
        "optimizerD = optim.RMSprop(disc.parameters(), lr = 0.00005)\n",
        "optimizerG = optim.RMSprop(gen.parameters(), lr = 0.00005)\n",
        "\n",
        "startFrom = gen.iterCount\n",
        "for epoch in range(startFrom,nIter):\n",
        "    disc.iterCount = disc.iterCount+1 \n",
        "    gen.iterCount = gen.iterCount+1\n",
        "    \n",
        "    for i, data in enumerate(dataloader, 0):      \n",
        "      for _ in range(3):\n",
        "        for p in disc.parameters():\n",
        "          p.requires_grad = True             \n",
        "        for p in disc.parameters():\n",
        "          p.data.clamp_(-0.01,0.01) \n",
        "\n",
        "        disc.zero_grad()\n",
        "        \n",
        "        # 1.a Training the discriminator with a real image of the dataset\n",
        "        real, _ = data[0]\n",
        "        input = Variable(real).to(device) \n",
        "        #target = Variable(torch.ones(input.size()[0])).to(device) \n",
        "        output = disc(input).to(device) \n",
        "        errD_real = -torch.mean(output)\n",
        "        errD_real.backward()\n",
        "        \n",
        "        # 1.b Training the discriminator with a fake image generated by the generator\n",
        "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)).to(device)\n",
        "        fake = gen(noise).to(device) \n",
        "        #target = Variable(torch.zeros(input.size()[0])).to(device)\n",
        "        output = disc(fake.detach()).to(device) \n",
        "        errD_fake = torch.mean(output)\n",
        "        errD_fake.backward()\n",
        "        \n",
        "        # Backpropagating the total error\n",
        "        errD = errD_real - errD_fake\n",
        "        #errD.backward()\n",
        "        optimizerD.step()\n",
        "           \n",
        "        \n",
        "      # 2 Training the generator\n",
        "      for p in disc.parameters():\n",
        "          p.requires_grad = False # to avoid computation\n",
        "      gen.zero_grad()\n",
        "      noise = Variable(torch.randn(input.size()[0], 100, 1, 1)).to(device)\n",
        "      fake = gen(noise).to(device) \n",
        "      #target = Variable(torch.ones(input.size()[0])).to(device) \n",
        "      output = disc(fake).to(device) \n",
        "      errG = -torch.mean(output)\n",
        "      errG.backward(retain_graph=True)\n",
        "      optimizerG.step() \n",
        "        \n",
        "      print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, nIter, i, len(dataloader), errD.item(), errG.item()))\n",
        "        \n",
        "            #print(errG.item())\n",
        "    torch.save(gen,modelFolder+ 'generator_wg.pt')\n",
        "    torch.save(disc,modelFolder+ 'discriminator_wg.pt')\n",
        "        \n",
        "    #3 Printing output and saving images and models\n",
        "    \n",
        "    if epoch % 1 == 0:\n",
        "        Vutil.save_image(real, '%s/real_samples.png' % resultsFolder, normalize = True)\n",
        "        fake = gen(noise)\n",
        "        Vutil.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (resultsFolder, epoch), normalize = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[937/5000][0/62] Loss_D: -0.1899 Loss_G: 0.1060\n",
            "[937/5000][1/62] Loss_D: 0.4514 Loss_G: -0.7149\n",
            "[937/5000][2/62] Loss_D: 1.2145 Loss_G: 0.4228\n",
            "[937/5000][3/62] Loss_D: 0.8806 Loss_G: -0.5343\n",
            "[937/5000][4/62] Loss_D: 0.7470 Loss_G: -0.7554\n",
            "[937/5000][5/62] Loss_D: -1.3404 Loss_G: -0.3049\n",
            "[937/5000][6/62] Loss_D: -1.4977 Loss_G: -0.7144\n",
            "[937/5000][7/62] Loss_D: -1.1460 Loss_G: 0.1761\n",
            "[937/5000][8/62] Loss_D: 1.3313 Loss_G: 0.6446\n",
            "[937/5000][9/62] Loss_D: 0.2181 Loss_G: -0.0405\n",
            "[937/5000][10/62] Loss_D: 0.1030 Loss_G: -0.5108\n",
            "[937/5000][11/62] Loss_D: -0.2953 Loss_G: 0.7385\n",
            "[937/5000][12/62] Loss_D: -1.0487 Loss_G: 0.7087\n",
            "[937/5000][13/62] Loss_D: -0.6113 Loss_G: 0.7018\n",
            "[937/5000][14/62] Loss_D: -0.4355 Loss_G: 0.7309\n",
            "[937/5000][15/62] Loss_D: 0.1430 Loss_G: 0.3650\n",
            "[937/5000][16/62] Loss_D: 0.8771 Loss_G: -0.1088\n",
            "[937/5000][17/62] Loss_D: -0.4990 Loss_G: 0.6894\n",
            "[937/5000][18/62] Loss_D: -1.2150 Loss_G: 0.2946\n",
            "[937/5000][19/62] Loss_D: -0.0722 Loss_G: 0.6398\n",
            "[937/5000][20/62] Loss_D: -0.0245 Loss_G: 0.6608\n",
            "[937/5000][21/62] Loss_D: -0.0828 Loss_G: 0.6708\n",
            "[937/5000][22/62] Loss_D: 0.0602 Loss_G: 0.5768\n",
            "[937/5000][23/62] Loss_D: -1.0207 Loss_G: 0.6811\n",
            "[937/5000][24/62] Loss_D: 0.4306 Loss_G: -0.6502\n",
            "[937/5000][25/62] Loss_D: 0.8366 Loss_G: -0.1716\n",
            "[937/5000][26/62] Loss_D: -0.3961 Loss_G: 0.7161\n",
            "[937/5000][27/62] Loss_D: 0.0052 Loss_G: 0.5926\n",
            "[937/5000][28/62] Loss_D: -0.2164 Loss_G: 0.6956\n",
            "[937/5000][29/62] Loss_D: 0.1337 Loss_G: 0.5073\n",
            "[937/5000][30/62] Loss_D: -1.0192 Loss_G: 0.7117\n",
            "[937/5000][31/62] Loss_D: -0.9756 Loss_G: 0.6635\n",
            "[937/5000][32/62] Loss_D: 0.0503 Loss_G: 0.6718\n",
            "[937/5000][33/62] Loss_D: -0.0517 Loss_G: 0.6961\n",
            "[937/5000][34/62] Loss_D: -0.0086 Loss_G: 0.6676\n",
            "[937/5000][35/62] Loss_D: -0.0497 Loss_G: 0.6701\n",
            "[937/5000][36/62] Loss_D: -0.0429 Loss_G: 0.6991\n",
            "[937/5000][37/62] Loss_D: -0.0434 Loss_G: 0.6930\n",
            "[937/5000][38/62] Loss_D: -0.0484 Loss_G: 0.6845\n",
            "[937/5000][39/62] Loss_D: -0.0562 Loss_G: 0.6945\n",
            "[937/5000][40/62] Loss_D: -0.0532 Loss_G: 0.6915\n",
            "[937/5000][41/62] Loss_D: -0.0131 Loss_G: 0.6662\n",
            "[937/5000][42/62] Loss_D: -0.0623 Loss_G: 0.6720\n",
            "[937/5000][43/62] Loss_D: -0.0464 Loss_G: 0.6928\n",
            "[937/5000][44/62] Loss_D: -0.0017 Loss_G: 0.6386\n",
            "[937/5000][45/62] Loss_D: -0.0777 Loss_G: 0.6975\n",
            "[937/5000][46/62] Loss_D: -0.0421 Loss_G: 0.6908\n",
            "[937/5000][47/62] Loss_D: -0.0419 Loss_G: 0.6730\n",
            "[937/5000][48/62] Loss_D: -0.0612 Loss_G: 0.6923\n",
            "[937/5000][49/62] Loss_D: -0.0250 Loss_G: 0.6740\n",
            "[937/5000][50/62] Loss_D: 0.0019 Loss_G: 0.5923\n",
            "[937/5000][51/62] Loss_D: -0.0623 Loss_G: 0.7064\n",
            "[937/5000][52/62] Loss_D: -0.0309 Loss_G: 0.6872\n",
            "[937/5000][53/62] Loss_D: -0.0569 Loss_G: 0.7036\n",
            "[937/5000][54/62] Loss_D: -0.0490 Loss_G: 0.7018\n",
            "[937/5000][55/62] Loss_D: -0.0289 Loss_G: 0.6841\n",
            "[937/5000][56/62] Loss_D: -0.0501 Loss_G: 0.7087\n",
            "[937/5000][57/62] Loss_D: -0.0436 Loss_G: 0.7002\n",
            "[937/5000][58/62] Loss_D: 0.0173 Loss_G: 0.5858\n",
            "[937/5000][59/62] Loss_D: -0.7246 Loss_G: 0.6997\n",
            "[937/5000][60/62] Loss_D: 0.4510 Loss_G: -0.5795\n",
            "[937/5000][61/62] Loss_D: 0.9613 Loss_G: -0.6450\n",
            "[938/5000][0/62] Loss_D: 0.6106 Loss_G: -0.7387\n",
            "[938/5000][1/62] Loss_D: -1.4037 Loss_G: -0.5569\n",
            "[938/5000][2/62] Loss_D: -0.4527 Loss_G: 0.5094\n",
            "[938/5000][3/62] Loss_D: 0.8544 Loss_G: -0.3666\n",
            "[938/5000][4/62] Loss_D: -0.2304 Loss_G: 0.6923\n",
            "[938/5000][5/62] Loss_D: 0.0241 Loss_G: 0.6905\n",
            "[938/5000][6/62] Loss_D: -0.0424 Loss_G: 0.6946\n",
            "[938/5000][7/62] Loss_D: -0.0436 Loss_G: 0.6902\n",
            "[938/5000][8/62] Loss_D: -0.0390 Loss_G: 0.6963\n",
            "[938/5000][9/62] Loss_D: -0.0450 Loss_G: 0.6964\n",
            "[938/5000][10/62] Loss_D: -0.0373 Loss_G: 0.6918\n",
            "[938/5000][11/62] Loss_D: -0.0432 Loss_G: 0.6899\n",
            "[938/5000][12/62] Loss_D: -0.0470 Loss_G: 0.7003\n",
            "[938/5000][13/62] Loss_D: -0.0472 Loss_G: 0.7095\n",
            "[938/5000][14/62] Loss_D: -0.0464 Loss_G: 0.6976\n",
            "[938/5000][15/62] Loss_D: -0.0477 Loss_G: 0.7086\n",
            "[938/5000][16/62] Loss_D: 0.0041 Loss_G: 0.6520\n",
            "[938/5000][17/62] Loss_D: -0.0507 Loss_G: 0.6966\n",
            "[938/5000][18/62] Loss_D: -0.0403 Loss_G: 0.6958\n",
            "[938/5000][19/62] Loss_D: -0.0609 Loss_G: 0.7092\n",
            "[938/5000][20/62] Loss_D: -0.0127 Loss_G: 0.6807\n",
            "[938/5000][21/62] Loss_D: -0.0545 Loss_G: 0.7075\n",
            "[938/5000][22/62] Loss_D: -0.0426 Loss_G: 0.6990\n",
            "[938/5000][23/62] Loss_D: 0.0354 Loss_G: 0.5671\n",
            "[938/5000][24/62] Loss_D: -0.1898 Loss_G: 0.7164\n",
            "[938/5000][25/62] Loss_D: 1.1441 Loss_G: -0.3080\n",
            "[938/5000][26/62] Loss_D: -0.9262 Loss_G: 0.6382\n",
            "[938/5000][27/62] Loss_D: 0.4622 Loss_G: -0.4979\n",
            "[938/5000][28/62] Loss_D: 0.2260 Loss_G: -0.0819\n",
            "[938/5000][29/62] Loss_D: -1.4917 Loss_G: -0.6351\n",
            "[938/5000][30/62] Loss_D: 0.9559 Loss_G: 0.1578\n",
            "[938/5000][31/62] Loss_D: -1.3417 Loss_G: 0.2143\n",
            "[938/5000][32/62] Loss_D: -1.0727 Loss_G: 0.6794\n",
            "[938/5000][33/62] Loss_D: -0.0407 Loss_G: 0.6854\n",
            "[938/5000][34/62] Loss_D: -0.0505 Loss_G: 0.6847\n",
            "[938/5000][35/62] Loss_D: -0.0473 Loss_G: 0.6990\n",
            "[938/5000][36/62] Loss_D: -0.0470 Loss_G: 0.6985\n",
            "[938/5000][37/62] Loss_D: -0.0210 Loss_G: 0.6935\n",
            "[938/5000][38/62] Loss_D: -0.0435 Loss_G: 0.7079\n",
            "[938/5000][39/62] Loss_D: -0.0107 Loss_G: 0.6700\n",
            "[938/5000][40/62] Loss_D: -0.0464 Loss_G: 0.7093\n",
            "[938/5000][41/62] Loss_D: -0.0494 Loss_G: 0.6961\n",
            "[938/5000][42/62] Loss_D: -0.0312 Loss_G: 0.6884\n",
            "[938/5000][43/62] Loss_D: -0.0490 Loss_G: 0.7021\n",
            "[938/5000][44/62] Loss_D: -0.0296 Loss_G: 0.6902\n",
            "[938/5000][45/62] Loss_D: -0.0491 Loss_G: 0.7045\n",
            "[938/5000][46/62] Loss_D: -0.0466 Loss_G: 0.6996\n",
            "[938/5000][47/62] Loss_D: -0.0509 Loss_G: 0.6955\n",
            "[938/5000][48/62] Loss_D: -0.0460 Loss_G: 0.7032\n",
            "[938/5000][49/62] Loss_D: -0.0163 Loss_G: 0.6672\n",
            "[938/5000][50/62] Loss_D: -0.0390 Loss_G: 0.6891\n",
            "[938/5000][51/62] Loss_D: -0.0418 Loss_G: 0.6783\n",
            "[938/5000][52/62] Loss_D: -0.0542 Loss_G: 0.7131\n",
            "[938/5000][53/62] Loss_D: -0.0472 Loss_G: 0.7026\n",
            "[938/5000][54/62] Loss_D: -0.0513 Loss_G: 0.7014\n",
            "[938/5000][55/62] Loss_D: -0.0412 Loss_G: 0.6942\n",
            "[938/5000][56/62] Loss_D: -0.0507 Loss_G: 0.7029\n",
            "[938/5000][57/62] Loss_D: -0.0508 Loss_G: 0.7037\n",
            "[938/5000][58/62] Loss_D: -0.0516 Loss_G: 0.7006\n",
            "[938/5000][59/62] Loss_D: -0.0549 Loss_G: 0.6988\n",
            "[938/5000][60/62] Loss_D: -0.0360 Loss_G: 0.6800\n",
            "[938/5000][61/62] Loss_D: -0.0619 Loss_G: 0.6993\n",
            "[939/5000][0/62] Loss_D: -0.0450 Loss_G: 0.7066\n",
            "[939/5000][1/62] Loss_D: -0.0177 Loss_G: 0.6739\n",
            "[939/5000][2/62] Loss_D: -0.0590 Loss_G: 0.7043\n",
            "[939/5000][3/62] Loss_D: -0.0121 Loss_G: 0.6753\n",
            "[939/5000][4/62] Loss_D: -0.0676 Loss_G: 0.6973\n",
            "[939/5000][5/62] Loss_D: -0.0405 Loss_G: 0.7034\n",
            "[939/5000][6/62] Loss_D: -0.0350 Loss_G: 0.6828\n",
            "[939/5000][7/62] Loss_D: -0.0581 Loss_G: 0.7048\n",
            "[939/5000][8/62] Loss_D: 0.0174 Loss_G: 0.6298\n",
            "[939/5000][9/62] Loss_D: -0.1261 Loss_G: 0.7035\n",
            "[939/5000][10/62] Loss_D: 0.0289 Loss_G: 0.5901\n",
            "[939/5000][11/62] Loss_D: -0.2093 Loss_G: 0.6994\n",
            "[939/5000][12/62] Loss_D: -0.0307 Loss_G: 0.7205\n",
            "[939/5000][13/62] Loss_D: 0.0302 Loss_G: 0.6370\n",
            "[939/5000][14/62] Loss_D: -0.0852 Loss_G: 0.7159\n",
            "[939/5000][15/62] Loss_D: -0.0303 Loss_G: 0.6915\n",
            "[939/5000][16/62] Loss_D: -0.0484 Loss_G: 0.6937\n",
            "[939/5000][17/62] Loss_D: -0.0496 Loss_G: 0.6872\n",
            "[939/5000][18/62] Loss_D: -0.0396 Loss_G: 0.6938\n",
            "[939/5000][19/62] Loss_D: 0.0700 Loss_G: 0.3011\n",
            "[939/5000][20/62] Loss_D: -1.4713 Loss_G: -0.6601\n",
            "[939/5000][21/62] Loss_D: -0.2290 Loss_G: 0.5764\n",
            "[939/5000][22/62] Loss_D: 0.9887 Loss_G: -0.5157\n",
            "[939/5000][23/62] Loss_D: -0.0930 Loss_G: 0.7030\n",
            "[939/5000][24/62] Loss_D: -0.0268 Loss_G: 0.6921\n",
            "[939/5000][25/62] Loss_D: -0.0269 Loss_G: 0.6668\n",
            "[939/5000][26/62] Loss_D: -0.0497 Loss_G: 0.6914\n",
            "[939/5000][27/62] Loss_D: -0.0111 Loss_G: 0.6526\n",
            "[939/5000][28/62] Loss_D: -0.0546 Loss_G: 0.7089\n",
            "[939/5000][29/62] Loss_D: -0.0512 Loss_G: 0.7130\n",
            "[939/5000][30/62] Loss_D: -0.0475 Loss_G: 0.7114\n",
            "[939/5000][31/62] Loss_D: -0.0254 Loss_G: 0.6907\n",
            "[939/5000][32/62] Loss_D: -0.0456 Loss_G: 0.7100\n",
            "[939/5000][33/62] Loss_D: -0.0456 Loss_G: 0.7155\n",
            "[939/5000][34/62] Loss_D: 0.0502 Loss_G: 0.5413\n",
            "[939/5000][35/62] Loss_D: -1.5077 Loss_G: -0.7034\n",
            "[939/5000][36/62] Loss_D: -0.3334 Loss_G: 0.6813\n",
            "[939/5000][37/62] Loss_D: -1.2765 Loss_G: 0.5723\n",
            "[939/5000][38/62] Loss_D: -0.1201 Loss_G: 0.6891\n",
            "[939/5000][39/62] Loss_D: 0.2019 Loss_G: -0.1565\n",
            "[939/5000][40/62] Loss_D: -1.4835 Loss_G: -0.6906\n",
            "[939/5000][41/62] Loss_D: -1.1614 Loss_G: 0.1346\n",
            "[939/5000][42/62] Loss_D: -1.1316 Loss_G: 0.6726\n",
            "[939/5000][43/62] Loss_D: 0.3488 Loss_G: 0.0002\n",
            "[939/5000][44/62] Loss_D: -0.2686 Loss_G: 0.7115\n",
            "[939/5000][45/62] Loss_D: -0.4733 Loss_G: 0.7135\n",
            "[939/5000][46/62] Loss_D: 0.1346 Loss_G: 0.5007\n",
            "[939/5000][47/62] Loss_D: -0.0649 Loss_G: 0.6810\n",
            "[939/5000][48/62] Loss_D: -0.0432 Loss_G: 0.6865\n",
            "[939/5000][49/62] Loss_D: -0.0495 Loss_G: 0.6915\n",
            "[939/5000][50/62] Loss_D: -0.0477 Loss_G: 0.6954\n",
            "[939/5000][51/62] Loss_D: -0.0415 Loss_G: 0.7003\n",
            "[939/5000][52/62] Loss_D: -0.0463 Loss_G: 0.6947\n",
            "[939/5000][53/62] Loss_D: -0.0461 Loss_G: 0.7032\n",
            "[939/5000][54/62] Loss_D: -0.0469 Loss_G: 0.7059\n",
            "[939/5000][55/62] Loss_D: -0.0445 Loss_G: 0.6959\n",
            "[939/5000][56/62] Loss_D: -0.0474 Loss_G: 0.7005\n",
            "[939/5000][57/62] Loss_D: -0.0545 Loss_G: 0.7084\n",
            "[939/5000][58/62] Loss_D: -0.0075 Loss_G: 0.6708\n",
            "[939/5000][59/62] Loss_D: -0.0446 Loss_G: 0.7028\n",
            "[939/5000][60/62] Loss_D: -0.0398 Loss_G: 0.6868\n",
            "[939/5000][61/62] Loss_D: -0.0498 Loss_G: 0.7016\n",
            "[940/5000][0/62] Loss_D: -0.0497 Loss_G: 0.7021\n",
            "[940/5000][1/62] Loss_D: -0.0509 Loss_G: 0.7063\n",
            "[940/5000][2/62] Loss_D: -0.0533 Loss_G: 0.7135\n",
            "[940/5000][3/62] Loss_D: -0.0390 Loss_G: 0.7029\n",
            "[940/5000][4/62] Loss_D: -0.0432 Loss_G: 0.7008\n",
            "[940/5000][5/62] Loss_D: -0.0432 Loss_G: 0.7047\n",
            "[940/5000][6/62] Loss_D: -0.0493 Loss_G: 0.6996\n",
            "[940/5000][7/62] Loss_D: -0.0515 Loss_G: 0.7011\n",
            "[940/5000][8/62] Loss_D: -0.0459 Loss_G: 0.7008\n",
            "[940/5000][9/62] Loss_D: -0.0438 Loss_G: 0.6887\n",
            "[940/5000][10/62] Loss_D: -0.0492 Loss_G: 0.7108\n",
            "[940/5000][11/62] Loss_D: -0.0487 Loss_G: 0.7055\n",
            "[940/5000][12/62] Loss_D: -0.0308 Loss_G: 0.6850\n",
            "[940/5000][13/62] Loss_D: -0.0556 Loss_G: 0.7006\n",
            "[940/5000][14/62] Loss_D: -0.0407 Loss_G: 0.7025\n",
            "[940/5000][15/62] Loss_D: -0.0374 Loss_G: 0.6873\n",
            "[940/5000][16/62] Loss_D: -0.0513 Loss_G: 0.7028\n",
            "[940/5000][17/62] Loss_D: -0.0352 Loss_G: 0.6809\n",
            "[940/5000][18/62] Loss_D: -0.0550 Loss_G: 0.7130\n",
            "[940/5000][19/62] Loss_D: -0.0360 Loss_G: 0.6989\n",
            "[940/5000][20/62] Loss_D: -0.0422 Loss_G: 0.7104\n",
            "[940/5000][21/62] Loss_D: -0.0438 Loss_G: 0.7062\n",
            "[940/5000][22/62] Loss_D: -0.0487 Loss_G: 0.7150\n",
            "[940/5000][23/62] Loss_D: -0.0443 Loss_G: 0.7153\n",
            "[940/5000][24/62] Loss_D: -0.0291 Loss_G: 0.6874\n",
            "[940/5000][25/62] Loss_D: -0.0521 Loss_G: 0.6690\n",
            "[940/5000][26/62] Loss_D: -0.0196 Loss_G: 0.6691\n",
            "[940/5000][27/62] Loss_D: -0.0670 Loss_G: 0.7090\n",
            "[940/5000][28/62] Loss_D: 0.0714 Loss_G: 0.2581\n",
            "[940/5000][29/62] Loss_D: -0.0915 Loss_G: 0.7033\n",
            "[940/5000][30/62] Loss_D: -1.3078 Loss_G: 0.2576\n",
            "[940/5000][31/62] Loss_D: -1.3413 Loss_G: 0.1898\n",
            "[940/5000][32/62] Loss_D: -0.3176 Loss_G: 0.7135\n",
            "[940/5000][33/62] Loss_D: -1.3817 Loss_G: -0.2865\n",
            "[940/5000][34/62] Loss_D: -1.3069 Loss_G: 0.5061\n",
            "[940/5000][35/62] Loss_D: -0.5922 Loss_G: 0.6990\n",
            "[940/5000][36/62] Loss_D: -1.1324 Loss_G: 0.5350\n",
            "[940/5000][37/62] Loss_D: 0.1949 Loss_G: 0.2752\n",
            "[940/5000][38/62] Loss_D: -1.1221 Loss_G: 0.6799\n",
            "[940/5000][39/62] Loss_D: 0.1490 Loss_G: 0.6216\n",
            "[940/5000][40/62] Loss_D: -0.0446 Loss_G: 0.6929\n",
            "[940/5000][41/62] Loss_D: -0.0504 Loss_G: 0.7075\n",
            "[940/5000][42/62] Loss_D: -0.0541 Loss_G: 0.7076\n",
            "[940/5000][43/62] Loss_D: -0.0544 Loss_G: 0.7046\n",
            "[940/5000][44/62] Loss_D: -0.0469 Loss_G: 0.6860\n",
            "[940/5000][45/62] Loss_D: -0.0475 Loss_G: 0.6966\n",
            "[940/5000][46/62] Loss_D: -0.0407 Loss_G: 0.6916\n",
            "[940/5000][47/62] Loss_D: -0.0514 Loss_G: 0.6928\n",
            "[940/5000][48/62] Loss_D: -0.0475 Loss_G: 0.7013\n",
            "[940/5000][49/62] Loss_D: -0.0502 Loss_G: 0.6912\n",
            "[940/5000][50/62] Loss_D: -0.0510 Loss_G: 0.7026\n",
            "[940/5000][51/62] Loss_D: -0.0443 Loss_G: 0.6969\n",
            "[940/5000][52/62] Loss_D: -0.0469 Loss_G: 0.7061\n",
            "[940/5000][53/62] Loss_D: -0.0483 Loss_G: 0.7151\n",
            "[940/5000][54/62] Loss_D: -0.0271 Loss_G: 0.6905\n",
            "[940/5000][55/62] Loss_D: -0.0513 Loss_G: 0.7054\n",
            "[940/5000][56/62] Loss_D: -0.0524 Loss_G: 0.7056\n",
            "[940/5000][57/62] Loss_D: -0.0432 Loss_G: 0.6982\n",
            "[940/5000][58/62] Loss_D: -0.0494 Loss_G: 0.6937\n",
            "[940/5000][59/62] Loss_D: -0.0393 Loss_G: 0.6852\n",
            "[940/5000][60/62] Loss_D: -0.0457 Loss_G: 0.7102\n",
            "[940/5000][61/62] Loss_D: -0.0393 Loss_G: 0.7046\n",
            "[941/5000][0/62] Loss_D: -0.0472 Loss_G: 0.7161\n",
            "[941/5000][1/62] Loss_D: -0.0481 Loss_G: 0.7020\n",
            "[941/5000][2/62] Loss_D: -0.0467 Loss_G: 0.7100\n",
            "[941/5000][3/62] Loss_D: -0.0548 Loss_G: 0.7112\n",
            "[941/5000][4/62] Loss_D: -0.0458 Loss_G: 0.7139\n",
            "[941/5000][5/62] Loss_D: -0.0383 Loss_G: 0.7018\n",
            "[941/5000][6/62] Loss_D: -0.0418 Loss_G: 0.6884\n",
            "[941/5000][7/62] Loss_D: -0.0492 Loss_G: 0.7057\n",
            "[941/5000][8/62] Loss_D: -0.0440 Loss_G: 0.6971\n",
            "[941/5000][9/62] Loss_D: -0.0440 Loss_G: 0.6727\n",
            "[941/5000][10/62] Loss_D: -0.0335 Loss_G: 0.6584\n",
            "[941/5000][11/62] Loss_D: -0.0542 Loss_G: 0.7136\n",
            "[941/5000][12/62] Loss_D: -0.0196 Loss_G: 0.6839\n",
            "[941/5000][13/62] Loss_D: -0.0537 Loss_G: 0.6895\n",
            "[941/5000][14/62] Loss_D: -0.0277 Loss_G: 0.6707\n",
            "[941/5000][15/62] Loss_D: -0.0521 Loss_G: 0.7059\n",
            "[941/5000][16/62] Loss_D: -0.0360 Loss_G: 0.7018\n",
            "[941/5000][17/62] Loss_D: -0.0393 Loss_G: 0.6978\n",
            "[941/5000][18/62] Loss_D: -0.0536 Loss_G: 0.6981\n",
            "[941/5000][19/62] Loss_D: -0.0496 Loss_G: 0.7129\n",
            "[941/5000][20/62] Loss_D: -0.0394 Loss_G: 0.7027\n",
            "[941/5000][21/62] Loss_D: -0.0315 Loss_G: 0.6857\n",
            "[941/5000][22/62] Loss_D: -0.0610 Loss_G: 0.7110\n",
            "[941/5000][23/62] Loss_D: -0.0468 Loss_G: 0.7187\n",
            "[941/5000][24/62] Loss_D: -0.0230 Loss_G: 0.6822\n",
            "[941/5000][25/62] Loss_D: -0.0769 Loss_G: 0.6996\n",
            "[941/5000][26/62] Loss_D: 0.1237 Loss_G: -0.1591\n",
            "[941/5000][27/62] Loss_D: -1.4639 Loss_G: -0.5267\n",
            "[941/5000][28/62] Loss_D: 0.8956 Loss_G: -0.7008\n",
            "[941/5000][29/62] Loss_D: -1.1242 Loss_G: 0.3906\n",
            "[941/5000][30/62] Loss_D: -0.2720 Loss_G: 0.7146\n",
            "[941/5000][31/62] Loss_D: -1.4730 Loss_G: -0.6034\n",
            "[941/5000][32/62] Loss_D: 1.0246 Loss_G: -0.2965\n",
            "[941/5000][33/62] Loss_D: -0.2437 Loss_G: 0.7160\n",
            "[941/5000][34/62] Loss_D: -0.3933 Loss_G: 0.7102\n",
            "[941/5000][35/62] Loss_D: 0.7227 Loss_G: -0.7364\n",
            "[941/5000][36/62] Loss_D: -1.4386 Loss_G: -0.6199\n",
            "[941/5000][37/62] Loss_D: -0.3738 Loss_G: 0.6355\n",
            "[941/5000][38/62] Loss_D: -0.1337 Loss_G: 0.6875\n",
            "[941/5000][39/62] Loss_D: 0.1375 Loss_G: 0.4785\n",
            "[941/5000][40/62] Loss_D: -1.3350 Loss_G: 0.4377\n",
            "[941/5000][41/62] Loss_D: -0.8653 Loss_G: 0.6990\n",
            "[941/5000][42/62] Loss_D: -0.1406 Loss_G: 0.7007\n",
            "[941/5000][43/62] Loss_D: 0.0017 Loss_G: 0.6929\n",
            "[941/5000][44/62] Loss_D: -0.0308 Loss_G: 0.6849\n",
            "[941/5000][45/62] Loss_D: -0.0561 Loss_G: 0.7019\n",
            "[941/5000][46/62] Loss_D: -0.0502 Loss_G: 0.7023\n",
            "[941/5000][47/62] Loss_D: -0.0428 Loss_G: 0.6978\n",
            "[941/5000][48/62] Loss_D: -0.0424 Loss_G: 0.7069\n",
            "[941/5000][49/62] Loss_D: -0.0488 Loss_G: 0.7024\n",
            "[941/5000][50/62] Loss_D: -0.0472 Loss_G: 0.7048\n",
            "[941/5000][51/62] Loss_D: -0.0493 Loss_G: 0.7022\n",
            "[941/5000][52/62] Loss_D: -0.0464 Loss_G: 0.7123\n",
            "[941/5000][53/62] Loss_D: -0.0441 Loss_G: 0.7102\n",
            "[941/5000][54/62] Loss_D: -0.0503 Loss_G: 0.7081\n",
            "[941/5000][55/62] Loss_D: -0.0505 Loss_G: 0.7109\n",
            "[941/5000][56/62] Loss_D: 0.0148 Loss_G: 0.6497\n",
            "[941/5000][57/62] Loss_D: -0.0504 Loss_G: 0.7093\n",
            "[941/5000][58/62] Loss_D: -0.0440 Loss_G: 0.7113\n",
            "[941/5000][59/62] Loss_D: -0.0522 Loss_G: 0.6996\n",
            "[941/5000][60/62] Loss_D: -0.0498 Loss_G: 0.7125\n",
            "[941/5000][61/62] Loss_D: -0.0534 Loss_G: 0.7095\n",
            "[942/5000][0/62] Loss_D: -0.0479 Loss_G: 0.7063\n",
            "[942/5000][1/62] Loss_D: -0.0457 Loss_G: 0.7016\n",
            "[942/5000][2/62] Loss_D: -0.0355 Loss_G: 0.6940\n",
            "[942/5000][3/62] Loss_D: -0.0557 Loss_G: 0.7114\n",
            "[942/5000][4/62] Loss_D: -0.0454 Loss_G: 0.7147\n",
            "[942/5000][5/62] Loss_D: -0.0360 Loss_G: 0.6966\n",
            "[942/5000][6/62] Loss_D: -0.0337 Loss_G: 0.6680\n",
            "[942/5000][7/62] Loss_D: -0.0470 Loss_G: 0.7124\n",
            "[942/5000][8/62] Loss_D: -0.0207 Loss_G: 0.6795\n",
            "[942/5000][9/62] Loss_D: -0.0587 Loss_G: 0.6921\n",
            "[942/5000][10/62] Loss_D: -0.0434 Loss_G: 0.7086\n",
            "[942/5000][11/62] Loss_D: -0.0134 Loss_G: 0.6650\n",
            "[942/5000][12/62] Loss_D: -0.0726 Loss_G: 0.7060\n",
            "[942/5000][13/62] Loss_D: -0.0390 Loss_G: 0.6951\n",
            "[942/5000][14/62] Loss_D: -0.0541 Loss_G: 0.7047\n",
            "[942/5000][15/62] Loss_D: -0.0515 Loss_G: 0.7023\n",
            "[942/5000][16/62] Loss_D: -0.0399 Loss_G: 0.6915\n",
            "[942/5000][17/62] Loss_D: -0.0528 Loss_G: 0.7054\n",
            "[942/5000][18/62] Loss_D: -0.0495 Loss_G: 0.6971\n",
            "[942/5000][19/62] Loss_D: -0.0367 Loss_G: 0.6971\n",
            "[942/5000][20/62] Loss_D: -0.0248 Loss_G: 0.6795\n",
            "[942/5000][21/62] Loss_D: -0.0566 Loss_G: 0.7070\n",
            "[942/5000][22/62] Loss_D: 0.1024 Loss_G: 0.1514\n",
            "[942/5000][23/62] Loss_D: -1.4272 Loss_G: -0.4651\n",
            "[942/5000][24/62] Loss_D: 0.0011 Loss_G: 0.4445\n",
            "[942/5000][25/62] Loss_D: -1.4945 Loss_G: -0.6656\n",
            "[942/5000][26/62] Loss_D: -0.2351 Loss_G: 0.6791\n",
            "[942/5000][27/62] Loss_D: -0.5595 Loss_G: 0.7224\n",
            "[942/5000][28/62] Loss_D: 0.3922 Loss_G: -0.0218\n",
            "[942/5000][29/62] Loss_D: -0.2199 Loss_G: 0.7022\n",
            "[942/5000][30/62] Loss_D: -0.0360 Loss_G: 0.6911\n",
            "[942/5000][31/62] Loss_D: -0.0070 Loss_G: 0.6862\n",
            "[942/5000][32/62] Loss_D: -0.0449 Loss_G: 0.7144\n",
            "[942/5000][33/62] Loss_D: -0.0070 Loss_G: 0.6752\n",
            "[942/5000][34/62] Loss_D: -0.0510 Loss_G: 0.7052\n",
            "[942/5000][35/62] Loss_D: -0.0464 Loss_G: 0.7005\n",
            "[942/5000][36/62] Loss_D: -0.0507 Loss_G: 0.7116\n",
            "[942/5000][37/62] Loss_D: -0.0454 Loss_G: 0.7065\n",
            "[942/5000][38/62] Loss_D: -0.0453 Loss_G: 0.7057\n",
            "[942/5000][39/62] Loss_D: -0.0430 Loss_G: 0.7082\n",
            "[942/5000][40/62] Loss_D: -0.0199 Loss_G: 0.6788\n",
            "[942/5000][41/62] Loss_D: -0.0489 Loss_G: 0.7088\n",
            "[942/5000][42/62] Loss_D: -0.0443 Loss_G: 0.7040\n",
            "[942/5000][43/62] Loss_D: -0.0433 Loss_G: 0.6928\n",
            "[942/5000][44/62] Loss_D: 0.1422 Loss_G: 0.1358\n",
            "[942/5000][45/62] Loss_D: -0.3721 Loss_G: 0.7281\n",
            "[942/5000][46/62] Loss_D: -1.1971 Loss_G: 0.6700\n",
            "[942/5000][47/62] Loss_D: -0.0112 Loss_G: 0.6955\n",
            "[942/5000][48/62] Loss_D: -0.0443 Loss_G: 0.7103\n",
            "[942/5000][49/62] Loss_D: -0.0205 Loss_G: 0.6864\n",
            "[942/5000][50/62] Loss_D: -0.0498 Loss_G: 0.6882\n",
            "[942/5000][51/62] Loss_D: -0.0481 Loss_G: 0.6936\n",
            "[942/5000][52/62] Loss_D: -0.0441 Loss_G: 0.7060\n",
            "[942/5000][53/62] Loss_D: -0.0499 Loss_G: 0.6992\n",
            "[942/5000][54/62] Loss_D: -0.0468 Loss_G: 0.6939\n",
            "[942/5000][55/62] Loss_D: -0.0529 Loss_G: 0.6995\n",
            "[942/5000][56/62] Loss_D: -0.0502 Loss_G: 0.7114\n",
            "[942/5000][57/62] Loss_D: -0.0411 Loss_G: 0.6929\n",
            "[942/5000][58/62] Loss_D: -0.0174 Loss_G: 0.6699\n",
            "[942/5000][59/62] Loss_D: -0.0488 Loss_G: 0.7132\n",
            "[942/5000][60/62] Loss_D: -0.0503 Loss_G: 0.6988\n",
            "[942/5000][61/62] Loss_D: -0.0485 Loss_G: 0.7148\n",
            "[943/5000][0/62] Loss_D: -0.0371 Loss_G: 0.7020\n",
            "[943/5000][1/62] Loss_D: -0.0443 Loss_G: 0.7152\n",
            "[943/5000][2/62] Loss_D: -0.0466 Loss_G: 0.7096\n",
            "[943/5000][3/62] Loss_D: -0.0422 Loss_G: 0.6983\n",
            "[943/5000][4/62] Loss_D: -0.0536 Loss_G: 0.6895\n",
            "[943/5000][5/62] Loss_D: -0.0464 Loss_G: 0.6934\n",
            "[943/5000][6/62] Loss_D: -0.0008 Loss_G: 0.6294\n",
            "[943/5000][7/62] Loss_D: -0.0772 Loss_G: 0.7129\n",
            "[943/5000][8/62] Loss_D: -0.0162 Loss_G: 0.6843\n",
            "[943/5000][9/62] Loss_D: -0.0445 Loss_G: 0.6783\n",
            "[943/5000][10/62] Loss_D: -0.0427 Loss_G: 0.6841\n",
            "[943/5000][11/62] Loss_D: -0.0556 Loss_G: 0.7051\n",
            "[943/5000][12/62] Loss_D: -0.0295 Loss_G: 0.6802\n",
            "[943/5000][13/62] Loss_D: -0.0529 Loss_G: 0.7097\n",
            "[943/5000][14/62] Loss_D: -0.0424 Loss_G: 0.6901\n",
            "[943/5000][15/62] Loss_D: -0.0435 Loss_G: 0.7074\n",
            "[943/5000][16/62] Loss_D: 0.1593 Loss_G: -0.3638\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}